{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [],
   "source": [
    "import os\n",
    "#!git clone https://github.com/Trusted-AI/AIF360.git\n",
    "#%cd AIF360\n",
    "#!python setup.py install\n",
    "#!pip install aif360[all]\n",
    "\n",
    "#!pip install tempeh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [],
   "source": [
    "import os, sys, itertools\n",
    "import numpy as np\n",
    "sys.path.append(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "#!pip install tempeh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [],
   "source": [
    "#!apt-get install tempeh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [],
   "source": [
    "from aif360.metrics.mdss_classification_metric import MDSSClassificationMetric\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [],
   "source": [
    "# !pip install aif360[all]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [],
   "source": [
    "#!apt-get install aif360"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [],
   "source": [
    "# a = np.random.rand(6).reshape(2,3) + 10\n",
    "# a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "#np.argmax(a, axis=0)\n",
    "#legge per colonne e restituisce un vettore lungo quanto il numero di colonne specificando in ogni casella l'indice dell'elemento massimo\n",
    "#in quella colonna\n",
    "\n",
    "#ad esempio [1,0,1] significa che ho 3 colonne,\n",
    "#- scansiono la matrice per colonne\n",
    "#-  il valore massimo della prima colonna (0) si trova ll'indice 1 -> mat[riga: 1][colonna: 0]\n",
    "#- il valore massimo della seconda colonna (1) si trova all'indice 0-> mat[riga: 0][colonna: 1]\n",
    "#- il valore massimo della terza colonna (2) si trova all'indice 1 -> mat[riga: 1][colonna: 2]\n",
    "\n",
    "#np.argmax(a, axis=1)\n",
    "\n",
    "#Analogamente con axis = 1 leggo per righe per cui otterrò un vettore lungo quanto il numero delle righe presenti nella matrice\n",
    "\n",
    "#es. [2,2]:\n",
    "#- il valore massimo della prima riga (0) si trova all'indice 2 ->mat[0][2]\n",
    "#- il valore massimo della seconda riga (1) si trova all'indice 2 -> mat[1][2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics.mdss_classification_metric import MDSSClassificationMetric\n",
    "from aif360.metrics.mdss.ScoringFunctions.Bernoulli import Bernoulli\n",
    "\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
    "from aif360.algorithms.inprocessing.meta_fair_classifier import MetaFairClassifier\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex [1.0]\n",
      "ooo\n",
      "df[attr]  [0.0]\n",
      "priviliged values prima:  [1.0]\n",
      "priviliged values =vals:  [1.0]\n",
      "unprivileged_values prima [0.0]\n",
      "<class 'list'>\n",
      "unprivileged_values dopo [0.0]\n",
      "race [1.0]\n",
      "ooo\n",
      "df[attr]  [0.0]\n",
      "priviliged values prima:  [1.0]\n",
      "priviliged values =vals:  [1.0]\n",
      "unprivileged_values prima [0.0]\n",
      "<class 'list'>\n",
      "unprivileged_values dopo [0.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "dataset_orig = load_preproc_data_compas()\n",
    "\n",
    "female_group = [{'sex': 1}]\n",
    "male_group = [{'sex': 0}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [],
   "source": [
    "dataset_orig_df = pd.DataFrame(dataset_orig.features, columns=dataset_orig.feature_names)\n",
    "#print(dataset_orig_df.head())\n",
    "#features:    sex  race  age_cat=25 to 45  age_cat=Greater than 45  age_cat=Less than 25  \\\n",
    "#             priors_count=0  priors_count=1 to 3  priors_count=More than 3  \\\n",
    "#             c_charge_degree=F  c_charge_degree=M\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age_cat=Less than 25  age_cat=25 to 45  age_cat=Greater than 45\n",
      "0                      0.0               1.0                      0.0\n",
      "1                      1.0               0.0                      0.0\n",
      "2                      0.0               1.0                      0.0\n",
      "3                      0.0               1.0                      0.0\n",
      "4                      0.0               1.0                      0.0\n",
      "...                    ...               ...                      ...\n",
      "5273                   0.0               1.0                      0.0\n",
      "5274                   1.0               0.0                      0.0\n",
      "5275                   1.0               0.0                      0.0\n",
      "5276                   1.0               0.0                      0.0\n",
      "5277                   0.0               1.0                      0.0\n",
      "\n",
      "[5278 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45', 'age_cat=Greater than 45']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [],
   "source": [
    "a = np.argmax(dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45',\n",
    "                                     'age_cat=Greater than 45']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, ..., 0, 0, 1], dtype=int64)"
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.argmax(dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45',\n",
    "                                     'age_cat=Greater than 45']].values, axis = 1)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [],
   "source": [
    "\n",
    "#1. np.argmax restituisce l'indice dell valore massimo presente nella matrice\n",
    "\n",
    "#2. dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45', 'age_cat=Greater than 45']]\n",
    "\n",
    "#   Seleziono solo le colonne \"Less than 25\" etc che possono assumere solo valori di 0 o 1\n",
    "#   (Less than 25 = 1, la persona ha meno di 25 anni, altrimenti se 0 la persona ha più di 25 anni)\n",
    "\n",
    "#3. dataset_orig_df[['priors_count=0', 'priors_count=1 to 3','priors_count=More than 3']].values, axis = 1)\n",
    "#   Di tutto il dataset avente quelle 3 colonne seleziono solo gli elementi pari a 1 quindi siccome axis = 1\n",
    "#   leggo la matrice per righe e ottengo un vettore pari al numero di righe avente in ogni cella l'indice\n",
    "#   dell'elemento massimo appartenente a quella riga (teoricamente funziona anche senza mettere values)\n",
    "\n",
    "#4. age_cat = np.argmax(dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45','age_cat=Greater than 45']].values, axis=1).reshape(-1, 1)\n",
    "\n",
    "#   Mettendo 1 come secondo elemento a reshape vuol dire che voglio avere una singola colonna, dopodichè metto -1 che significa che la dimensione\n",
    "#   del numero di righe viene intuita automaticamente\n",
    "\n",
    "#   Nel nostro caso siccome abbiamo un vettore di 5728 elementi pari al numero di righe del dataset\n",
    "#   dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45', 'age_cat=Greater than 45']] facendo reshape(-1,1) stiamo dicendo\n",
    "#   di ottenere un nuovo vettore ma questa volta come vettore colonna il cui numero di righe intuito sarà uguale a 5728\n",
    "\n",
    "#   Ripeto lo stesso per gli attributi priors e charge_degree\n",
    "\n",
    "age_cat = np.argmax(dataset_orig_df[['age_cat=Less than 25', 'age_cat=25 to 45',\n",
    "                                     'age_cat=Greater than 45']].values, axis=1).reshape(-1, 1)\n",
    "priors_count = np.argmax(dataset_orig_df[['priors_count=0', 'priors_count=1 to 3',\n",
    "                                          'priors_count=More than 3']].values, axis=1).reshape(-1, 1)\n",
    "c_charge_degree = np.argmax(dataset_orig_df[['c_charge_degree=F', 'c_charge_degree=M']].values, axis=1).reshape(-1, 1)\n",
    "\n",
    "#   Concateno i vettori colonna insieme in modo tale da ottenere un dataset finale 5728x6\n",
    "features = np.concatenate((dataset_orig_df[['sex', 'race']].values, age_cat, priors_count, \\\n",
    "                           c_charge_degree, dataset_orig.labels), axis=1)\n",
    "feature_names = ['sex', 'race', 'age_cat', 'priors_count', 'c_charge_degree']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features, columns=feature_names + ['two_year_recid'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "data": {
      "text/plain": "   sex  race  age_cat  priors_count  c_charge_degree  two_year_recid\n0  0.0   0.0      1.0           0.0              0.0             1.0\n1  0.0   0.0      0.0           2.0              0.0             1.0\n2  0.0   1.0      1.0           2.0              0.0             1.0\n3  1.0   1.0      1.0           0.0              1.0             0.0\n4  0.0   1.0      1.0           0.0              0.0             0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sex</th>\n      <th>race</th>\n      <th>age_cat</th>\n      <th>priors_count</th>\n      <th>c_charge_degree</th>\n      <th>two_year_recid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Training**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex [1]\n",
      "ooo\n",
      "df[attr]  [0.0]\n",
      "priviliged values prima:  [1.0]\n",
      "priviliged values =vals:  [1]\n",
      "unprivileged_values prima [0.0]\n",
      "<class 'list'>\n",
      "unprivileged_values dopo [0.0]\n",
      "race [1]\n",
      "ooo\n",
      "df[attr]  [0.0]\n",
      "priviliged values prima:  [1.0]\n",
      "priviliged values =vals:  [1]\n",
      "unprivileged_values prima [0.0]\n",
      "<class 'list'>\n",
      "unprivileged_values dopo [0.0]\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import StandardDataset\n",
    "dataset = StandardDataset(df, label_name='two_year_recid', favorable_classes=[0],\n",
    "                 protected_attribute_names=['sex', 'race'],\n",
    "                 privileged_classes=[[1], [1]],\n",
    "                 instance_weights_name=None)\n",
    "\n",
    "#sex -> 1: male, 0: female\n",
    "#race -> 1: caucasian 0: not caucasian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [
    {
     "data": {
      "text/plain": "set()"
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {2}\n",
    "a.difference([2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Index.get_loc of Index(['sex', 'race', 'age_cat', 'priors_count', 'c_charge_degree',\n       'two_year_recid'],\n      dtype='object')>"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.get_loc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset.split([0.7], shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Training Dataset shape"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3694, 5)\n",
      "0.0 1.0\n",
      "['sex', 'race']\n",
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n",
      "['age_cat', 'c_charge_degree', 'priors_count', 'race', 'sex']\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Favorable and unfavorable labels"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Protected attribute names"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Privileged and unprivileged protected attribute values"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Dataset feature names"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes,\n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.124496\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.159410\n"
     ]
    }
   ],
   "source": [
    "metric_train = BinaryLabelDatasetMetric(dataset_orig_train,\n",
    "                             unprivileged_groups=male_group,\n",
    "                             privileged_groups=female_group)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_train.mean_difference())\n",
    "metric_test = BinaryLabelDatasetMetric(dataset_orig_test,\n",
    "                             unprivileged_groups=male_group,\n",
    "                             privileged_groups=female_group)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_test.mean_difference())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs', C=1.0, penalty='l2')\n",
    "clf.fit(dataset_orig_train.features, dataset_orig_train.labels.flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "dataset_bias_test_prob = clf.predict_proba(dataset_orig_test.features)[:,1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "dff = pd.DataFrame(dataset_orig_test.features, columns=dataset_orig_test.feature_names)\n",
    "dff['observed'] = pd.Series(dataset_orig_test.labels.flatten(), index=dff.index)\n",
    "dff['probabilities'] = pd.Series(dataset_bias_test_prob, index=dff.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "dataset_bias_test = dataset_orig_test.copy()\n",
    "dataset_bias_test.scores = dataset_bias_test_prob\n",
    "dataset_bias_test.labels = dataset_orig_test.labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Bias Scoring**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "mdss_classified = MDSSClassificationMetric(dataset_orig_test, dataset_bias_test,\n",
    "                         unprivileged_groups=male_group,\n",
    "                         privileged_groups=female_group)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [
    {
     "data": {
      "text/plain": "-1e-17"
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_privileged_score = mdss_classified.score_groups(privileged=True)\n",
    "female_privileged_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [
    {
     "data": {
      "text/plain": "-1e-17"
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_unprivileged_score = mdss_classified.score_groups(privileged=False)\n",
    "male_unprivileged_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [],
   "source": [
    "mdss_classified = MDSSClassificationMetric(dataset_orig_test, dataset_bias_test,\n",
    "                         unprivileged_groups=female_group,\n",
    "                         privileged_groups=male_group)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [
    {
     "data": {
      "text/plain": "0.630108034329993"
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_privileged_score = mdss_classified.score_groups(privileged=True)\n",
    "male_privileged_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "data": {
      "text/plain": "1.17706135776708"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_unprivileged_score = mdss_classified.score_groups(privileged=False)\n",
    "female_unprivileged_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##**Bias Scan**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "privileged_subset = mdss_classified.bias_scan(penalty=0.5, privileged=True)\n",
    "unprivileged_subset = mdss_classified.bias_scan(penalty=0.5, privileged=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'race': [0.0], 'sex': [0.0], 'age_cat': [0.0]}, 3.1531055108604917)\n",
      "({'sex': [1.0], 'race': [0.0]}, 3.303741512514005)\n"
     ]
    }
   ],
   "source": [
    "print(privileged_subset)\n",
    "print(unprivileged_subset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "assert privileged_subset[0]\n",
    "assert unprivileged_subset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "protected_attr_names = set(privileged_subset[0].keys()).union(set(unprivileged_subset[0].keys()))\n",
    "dataset_orig_test.protected_attribute_names = list(protected_attr_names)\n",
    "dataset_bias_test.protected_attribute_names = list(protected_attr_names)\n",
    "\n",
    "protected_attr = np.where(np.isin(dataset_orig_test.feature_names, list(protected_attr_names)))[0]\n",
    "\n",
    "dataset_orig_test.protected_attributes = dataset_orig_test.features[:, protected_attr]\n",
    "dataset_bias_test.protected_attributes = dataset_bias_test.features[:, protected_attr]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Training Dataset shape"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584, 5)\n",
      "0.0 1.0\n",
      "['race', 'sex', 'age_cat']\n",
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n",
      "['age_cat', 'c_charge_degree', 'priors_count', 'race', 'sex']\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Favorable and unfavorable labels"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Protected attribute names"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Privileged and unprivileged protected attribute values"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Dataset feature names"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_bias_test.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_bias_test.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_bias_test.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_bias_test.privileged_protected_attributes,\n",
    "      dataset_bias_test.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_bias_test.feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "# converts from dictionary of lists to list of dictionaries\n",
    "a = list(privileged_subset[0].values())\n",
    "subset_values = list(itertools.product(*a))\n",
    "\n",
    "detected_privileged_groups = []\n",
    "for vals in subset_values:\n",
    "    detected_privileged_groups.append((dict(zip(privileged_subset[0].keys(), vals))))\n",
    "\n",
    "a = list(unprivileged_subset[0].values())\n",
    "subset_values = list(itertools.product(*a))\n",
    "\n",
    "detected_unprivileged_groups = []\n",
    "for vals in subset_values:\n",
    "    detected_unprivileged_groups.append((dict(zip(unprivileged_subset[0].keys(), vals))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.211737\n"
     ]
    }
   ],
   "source": [
    "metric_bias_test = BinaryLabelDatasetMetric(dataset_bias_test,\n",
    "                                             unprivileged_groups=detected_unprivileged_groups,\n",
    "                                             privileged_groups=detected_privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\"\n",
    "      % metric_bias_test.mean_difference())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "to_choose = dff[privileged_subset[0].keys()].isin(privileged_subset[0]).all(axis=1)\n",
    "temp_df = dff.loc[to_choose]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [
    {
     "data": {
      "text/plain": "'Our detected priviledged group has a size of 192, we observe 0.6770833333333334 as the average risk of recidivism, but our model predicts 0.5730004938240804'"
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Our detected priviledged group has a size of {}, we observe {} as the average risk of recidivism, but our model predicts {}\"\\\n",
    ".format(len(temp_df), temp_df['observed'].mean(), temp_df['probabilities'].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "data": {
      "text/plain": "'This is a multiplicative increase in the odds by 1.56251443909305'"
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_obs = temp_df['observed'].mean()\n",
    "group_prob = temp_df['probabilities'].mean()\n",
    "\n",
    "odds_mul = (group_obs / (1 - group_obs)) / (group_prob /(1 - group_prob))\n",
    "\"This is a multiplicative increase in the odds by {}\"\\\n",
    ".format(odds_mul)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [],
   "source": [
    "assert odds_mul > 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "to_choose = dff[unprivileged_subset[0].keys()].isin(unprivileged_subset[0]).all(axis=1)\n",
    "temp_df = dff.loc[to_choose]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [
    {
     "data": {
      "text/plain": "'Our detected unpriviledged group has a size of 169, we observe 0.33136094674556216 as the average risk of recidivism, but our model predicts 0.43652313575727764'"
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Our detected unpriviledged group has a size of {}, we observe {} as the average risk of recidivism, but our model predicts {}\"\\\n",
    ".format(len(temp_df), temp_df['observed'].mean(), temp_df['probabilities'].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [
    {
     "data": {
      "text/plain": "'This is a multiplicative decrease in the odds by 0.6397030278261826'"
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_obs = temp_df['observed'].mean()\n",
    "group_prob = temp_df['probabilities'].mean()\n",
    "\n",
    "odds_mul = (group_obs / (1 - group_obs)) / (group_prob /(1 - group_prob))\n",
    "\"This is a multiplicative decrease in the odds by {}\"\\\n",
    ".format(odds_mul)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "assert odds_mul < 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}